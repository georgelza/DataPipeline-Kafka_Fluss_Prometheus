# Building a Factory machine/sensor monitoring IoT Pipeline. From source through to Prometheus

## Overview

This originally started as a simple idea, create a IoT JSON packaged payload, publish it to a **Confluent Kafka Broker**, consume using a connector into Apache Flink and, do some fancy aggregation. 

But we tripped and ended in the proverbial rabbit hole, again, nothing new.

In this next installment of our IoT pipeline series, at a high level.

Source -> Apache Kafka -> Apache Flink -> Fluss -> Apache Flink and down into Prometheus, all displayed on Grafana.

1.  We will publish the payloads generated by the 6 factories grouped into 3 seperate **Kafka Topics** based on the regional locatation/distribution, namely `North`, `South` and `East`.

2.  Once on the Kafka Topics we will consume the payloads using a Kafka/Flink connector into `hive_catalog.kafka.factory_iot_#` tables. This is done using `<root>/devlab0/creFlinkFlows/2.1.creKafkaSource.sql`
   
3.  After this we will consolidate all into one fluss based table namely, `factory_iot_unnested` table, during this insert we also unnest/flatten the data structure.
   
4.  We will now create 3 flink/output tables using the "beta" prometheus flink sql connector. (see Build various containers, step 3 re the beta prometheus sink connector). 
   
5.  We will then run 3 jobs to insert the `siteId=101`, `siteId=102` and `siteId=103` into the above created tabless. During this step we assign the metric label, you can run all 3 flink inserts as one label or you can call it say **north_metrics**, **south_metrics** and **east_metrics**.
   
6.  At this point we have data flowing from source into kafka, via flink into fluss, then sinked down into prometheus using a flink connector. At this stage we now have the entire pipeline defined, allowing us to created fancy Grafana dashboards.

[GIT Repo:](https://github.com/georgelza/DataPipeline-Kafka_Fluss_Prometheus)

I have one more ToDo for propably the last installment of this series... publish onto Mosquito using MQTT protocol and source directly into Apache Flink, bypassing the Apache Kafka stack. A much more real world configuration.

Source -> Mosquito MQTT -> Apache Flink -> Fluss -> Apache Flink and down into Prometheus, all displayed on Grafana.


### NOTE:  Pretty important, Prometheus and time

The Prometheus container runs as UTC time (I've tried to use environment setting to change it, no success). But the following always applies, so make sure to consider that in anything re prometheus destined data.

1. Firstly Prometheus does not allow old data to be published onto the remote_writer that is to far in the past, similarly it also does not allow for time travellers from the future... Creating data with a time stamp to far into the future.

2. My data generator can be used in various configurations... i.e.: to just push metrics into Kafka and down into flink and onwards to Fluss. Where it can then be used to do analytics etc. on. In this scenario it can be configured to first create historic data and then go into an current mode.

3. However, if we are going to sink the data into Prometheus (as per this blog), be aware, as per above. Prometheus does not allow for data to be created in the past. To cater for this scenario... you need to set the `RUNHISTORIC=0` in the `<root>/app_iot#/site$.sh` file to disable the historic data generator functionality.

4. With Regard to back filling data for Prometheus, see: [backfilling-from-openmetrics-format](https://prometheus.io/docs/prometheus/latest/storage/#backfilling-from-openmetrics-format). For more see `<root>/otl/`


### Prometheus Sink connector

Ok, so before we go much further. This Prometheus sink connector uses the Prometheus servers remote writer framework. 

Baiscally this would normally be one prometheus host configured as a reciever and another configured to remote write (push) metrics to the receiver. 

For the Prometheus sink connector to work we configure our prometheus server to be a receiver. To eanble this on the receiver side the following environment variable `"--web.enable-remote-write-receiver"` is passed in. this then exposes a receiver process at `http://prometheus:9090/api/v1/write`

We then configure our Prometheus sink connector by setting `'metric.endpoint-url' = 'http://prometheus:9090/api/v1/write'`.

`http://prometheus:9090` being our receiver Prometheus server and `/api/v1/write` being the default url for the `remote-write-receiver`.


### Test Prometheus Stack

`<root>/devlab0/creFlinkFlows/2.0.crePromSource.sql` and `3.0.creProm*` are simple example of a table create and insert commands to see if all is working.


### Flink Catalog

We're still using our **Apache Hive Metastore** as catalog with a **PostgreSQL** database for backend storage.
See below for version information.


## Modules and Versions

- Ubuntu 24.04 LTS

- Confluent Kafka Cluster 7.9.1
  
- Apache Flink 1.20.1 - Java 17

- Apache Fluss 0.6.0  (With Zookeeper 3.9.2)

- Hadoop File System (HDFS) 3.3.5 build (OpenJDK11) on Ubuntu 24.04 LTS

- Hive Metastore 3.1.3 on Hadoop 3.3.5 (OpenJDK8) on Ubuntu 24.04 LTS

- PostgreSQL 12

- Python 3.13

- Prometheus v3.3.0
  
- Grafana 11.6.1


## Our various IoT Payloads formats.

### Basic min IoT Payload, produced by Factories 101 and 104

```json5
{
    "ts": 1729312551000, 
    "metadata": {
        "siteId": 101, 
        "deviceId": 1004, 
        "sensorId": 10034, 
        "unit": "BAR"
    }, 
    "measurement": 120
}
```

### Basic min IoT Payload, with a human readable time stamp & location added, produced by Factories 102 and 105

```json5
{
    "ts": 1713807946000, 
    "metadata": {
        "siteId": 102, 
        "deviceId": 1008, 
        "sensorId": 10073, 
        "unit": "Liter", 
        "ts_human": "2024-04-22T19:45:46.000000", 
        "location": {
            "latitude": -33.924869, 
            "longitude": 18.424055
        }
    }, 
    "measurement": 25
}
```

### Complete IoT Payload, with deviceType tag added, produced by Factories 103 and 106

```json5
{
    "ts": 1707882120000, 
    "metadata": {
        "siteId": 103, 
        "deviceId": 1014, 
        "sensorId": 10124, 
        "unit": "Amp", 
        "ts_human": "2024-02-14T05:42:00.000000", 
        "location": {
            "latitude": -33.9137, 
            "longitude": 25.5827
        }, 
        "deviceType": "Hoist_Motor"
    },
    "measurement": 24
}
```


## To run the project.

### See various configuration settings and passwords in:

0. devlab0/docker_compose.yml

1. .pwd in app_iot# in siteX.sh

2. devlab0/.env

3. devlab0/conf/hive.env

4. devlab0/conf/hive-site.xml


### Download containers and libraries

1. cd infrastructure

2. make pullall

3. make buildall


### Build various containers

1. cd devlab0

2. ./getlibs.sh

3. One lib/.jar file not downloadable at this stage, using the #2 command is our primary prometheus sink jar, this is because as we stand right now this is still in development and a PR, see: [flink-connector-prometheu](https://github.com/apache/flink-connector-prometheus/pull/22). As such I'm including it in the `<root>/devlab0/prometheus_jar` diretory, please copy/move this into the `<root>/devlab0/conf/flink/lib/flink` directory.

4. If you want to run the load generators as docker apps ->  make buildapp
   Otherwise skip step 4 and continue to 5.

5. Now, to run it please read README.md in `<root>/devlab0/README.md` file.


## Projects / Components

- [Apache Flink](https://flink.apache.org)

- [Ververica](https://www.ververica.com)

- [What is Fluss](https://alibaba.github.io/fluss-docs/)

- [Fluss Overview](https://alibaba.github.io/fluss-docs/docs/install-deploy/overview/)

- [What is Fluss docs](https://alibaba.github.io/fluss-docs/docs/intro/)

- [Fluss Project Git Repo](https://github.com/alibaba/fluss)

- [Introduction to Fluss](https://www.ververica.com/blog/introducing-fluss)

- [Apache Paimon](https://paimon.apache.org)

- [Apache Parquet File format](https://parquet.apache.org)


### Flink Libraries

As I am always travelling while writing these blog's and did not want to pull the libraries on every build I decided to downlaod them once into the below directory and then mount them into container. Just a different way less bandwidth and also slightly faster builds.

The `devlab0/conf/flink/lib/*` directories will house our Java libraries required by our Flink stack. 

Normally I'd include these in the Dockerfile as part of the image build, but during development it's easier if we place them here and then mount the directories into the containers at run time via our `docker-compose.yml` file inside the volume specification for the flink-* services.

This makes it simpler to add/remove libraries as we simply have to restart the flink container and not rebuild it.

Additionally, as the `jobmanager`, `taskmanager` use the same libraries doing it tis way allows us to use this one set, thus also reducing the disk space and the container image size.

The various files are downloaded by executing the `getlibs.sh` file located in the `devlab0/` directory.


### Flink base container images for 1.20.1 (manual pull from `hub.docker.com`)

- docker pull arm64v8/flink:1.20.1-scala_2.12-java11


### Self Build Flink container

- [Master Flink download](https://flink.apache.org/downloads/#apache-flink-1201)

- [Flink 1.20.1 binaries](https://www.apache.org/dyn/closer.lua/flink/flink-1.20.1/flink-1.20.1-bin-scala_2.12.tgz)


## Uncategorized notes and Articles

- [Apache Flink FLUSS](https://www.linkedin.com/posts/polyzos_fluss-is-now-open-source-activity-7268144336930832384-ds87?utm_source=share&utm_medium=member_desktop)


- [Apache Flink Deployment](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/)    
    

- [Troubleshooting Apache Flink SQL S3 problems](https://www.decodable.co/blog/troubleshooting-flink-sql-s3-problems)


### HDFS Cluster

- [Setting Up an HDFS Cluster with Docker Compose: A Step-by-Step Guide](https://bytemedirk.medium.com/setting-up-an-hdfs-cluster-with-docker-compose-a-step-by-step-guide-4541cd15b168)
- [Deploying Hadoop using Docker](https://medium.com/@garin.sanny07/hadoop-cluster-55477505d0ff)

- [Installing the Hadoop Stack using Docker](https://hackmd.io/@silicoflare/docker-hadoop)


### Flink Cluster

- [how-to-set-up-a-local-flink-cluster-using-docker](https://medium.com/marionete/how-to-set-up-a-local-flink-cluster-using-docker-0a0a741504f6)


### RocksDB

- [Using RocksDB State Backend in Apache Flink: When and How](https://flink.apache.org/2021/01/18/using-rocksdb-state-backend-in-apache-flink-when-and-how/)


### Log4J Logging levels

- [Log4J Logging Levels](https://logging.apache.org/log4j/2.x/manual/customloglevels.html)

- The Flink jobmanager and taskmanager log levels can be modified by editing the various `devlab0/conf/*.properties` files. Remember to restart your Flink containers.


### Great quick reference for docker compose

- [A Deep dive into Docker Compose by Alex Merced](https://dev.to/alexmercedcoder/a-deep-dive-into-docker-compose-27h5)


### Consider using secrets for sensitive information

- [How to use sectrets with Docker Compose](https://docs.docker.com/compose/how-tos/use-secrets/)


## Misc Notes

The various `REAMDE.md` utilises markdown syntax. You can refer to `https://markdownlivepreview.com` & `https://dillinger.io` for more information, examples.

To view a mardown file, `https://jumpshare.com/viewer/md` or if you using Visual Studio Code search for markdown as a module and try some of them.


### By:

George

[georgelza@gmail.com](georgelza@gmail.com)

[George on Linkedin](https://www.linkedin.com/in/george-leonard-945b502/)

[George on Medium](https://medium.com/@georgelza)

